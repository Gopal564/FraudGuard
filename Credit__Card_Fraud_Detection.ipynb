{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Setup and Downloading the Dataset\n",
        "\n",
        "### Setup Kaggle Environment\n",
        "\n",
        "1. Create a directory named `.kaggle` in your home directory:\n",
        "   ```bash\n",
        "   !mkdir ~/.kaggle\n",
        "  ```\n",
        "2. Copy the Kaggle API key file, kaggle.json, to the newly created directory:\n",
        "```\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "```\n",
        "3. Set the appropriate permissions for the API key file:\n",
        "```\n",
        "!chmod 600 ~/.kaggle/kaggle.json\n",
        "```\n",
        "### Download and Extract Dataset\n",
        "\n",
        "1. Use the Kaggle CLI to download the dataset titled \"Credit Card Fraud Detection\" by mlg-ulb:\n",
        "```\n",
        "!kaggle datasets download -d mlg-ulb/creditcardfraud\n",
        "```\n",
        "2. Unzip the downloaded file to extract its contents:\n",
        "```\n",
        "!unzip creditcardfraud.zip\n",
        "```\n"
      ],
      "metadata": {
        "id": "Ue7S37e9DCjx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WrNFAA8JAkO6"
      },
      "outputs": [],
      "source": [
        "!mkdir ~/.kaggle\n",
        "!cp /content/drive/MyDrive/Colab\\ Notebooks/kaggle.json /root/.kaggle\n",
        "!chmod 600 /root/.kaggle/kaggle.json\n",
        "!kaggle datasets download -d mlg-ulb/creditcardfraud -p /content/drive/MyDrive/Colab\\ Notebooks\n",
        "\n",
        "import zipfile\n",
        "zipfile_obj = zipfile.ZipFile(\"/content/drive/MyDrive/Colab Notebooks/creditcardfraud.zip\")\n",
        "zipfile_obj.extractall('/content/drive/MyDrive/Colab Notebooks')\n",
        "zipfile_obj.close()\n",
        "# !unzip creditcardfraud.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Read and Display Dataset\n",
        "The code uses the pandas library to read a CSV file named 'creditcard.csv' and displays the resulting dataset."
      ],
      "metadata": {
        "id": "HFKGNtxVDHTZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "# Read the CSV file into a DataFrame\n",
        "df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/creditcard.csv')\n",
        "# Display the DataFrame\n",
        "df"
      ],
      "metadata": {
        "id": "NJJp31JcDE8Z",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Class Distribution\n",
        "\n",
        "The output shows the count of each unique value in the 'Class' column.\n",
        "\n",
        "- Class 0: 284,315 occurrences\n",
        "- Class 1: 492 occurrences\n",
        "\n",
        "This information is valuable, especially in tasks like credit card fraud detection, where Class 1 often represents fraudulent transactions, and Class 0 represents non-fraudulent transactions. The imbalanced nature of the classes (with significantly more non-fraudulent transactions) is common in fraud detection datasets.\n",
        "\n"
      ],
      "metadata": {
        "id": "RJNITzAtUDsE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Count occurrences of each unique value in the 'Class' column\n",
        "df['Class'].value_counts()"
      ],
      "metadata": {
        "id": "OilLeoczD_Hx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Histograms for Numerical Columns\n",
        "\n",
        "The code generates histograms for each numerical column in the dataset."
      ],
      "metadata": {
        "id": "4jPvrL9_Uz1m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create histograms for each numerical column\n",
        "df.hist(bins=30, figsize=(30, 30))"
      ],
      "metadata": {
        "id": "pbVMNxexEOou"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Descriptive Statistics\n",
        "\n",
        "The code generates descriptive statistics for the numerical columns in the dataset.\n",
        "\n",
        "- The Amount min - 0 and max - 25691.16 and avg - 88.349619 which means their are few high values only\n",
        "- Class lies is either 0 or 1"
      ],
      "metadata": {
        "id": "MXatSlNkVeA_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.describe()"
      ],
      "metadata": {
        "id": "EXZ5p6N6EnZ_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Feature Scaling with RobustScaler and Min-Max Scaling\n",
        "\n",
        "The code uses the RobustScaler from scikit-learn to scale the 'Amount' column, addressing outliers. Additionally, it applies min-max scaling to the 'Time' column to normalize its values."
      ],
      "metadata": {
        "id": "KTJOamcgXQhr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import RobustScaler\n",
        "# Create a copy of the original DataFrame\n",
        "new_df = df.copy()\n",
        "# Scale the 'Amount' column using RobustScaler\n",
        "new_df['Amount'] = RobustScaler().fit_transform(new_df['Amount'].to_numpy().reshape(-1, 1))\n",
        "# Apply min-max scaling to the 'Time' column\n",
        "time = new_df['Time']\n",
        "new_df['Time'] = (time - time.min()) / (time.max() - time.min())\n",
        "# Display the resulting DataFrame\n",
        "new_df"
      ],
      "metadata": {
        "id": "IDutgtsEE2bM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Shuffling Rows in the DataFrame\n",
        "\n",
        "The code shuffles the rows of the DataFrame 'new_df' to introduce randomness."
      ],
      "metadata": {
        "id": "iebeY6VzX6S8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "new_df = new_df.sample(frac=1, random_state=1)\n",
        "new_df"
      ],
      "metadata": {
        "id": "Clo45ljJF5yw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train-Test-Validation Split and Class Distribution\n",
        "\n",
        "The code splits the shuffled DataFrame 'new_df' into three sets: training, testing, and validation.\n",
        "\n",
        "\n",
        "The class distribution is as follows:\n",
        "\n",
        "Training Set:\n",
        "\n",
        "- Class 0: 239,589 occurrences\n",
        "- Class 1: 411 occurrences\n",
        "\n",
        "Testing Set:\n",
        "\n",
        "- Class 0: 21,955 occurrences\n",
        "- Class 1: 45 occurrences\n",
        "\n",
        "Validation Set:\n",
        "\n",
        "- Class 0: 22,771 occurrences\n",
        "- Class 1: 36 occurrences\n",
        "\n",
        "These counts provide insights into the distribution of the target variable ('Class') in each set, which is crucial for understanding the balance between the two classes."
      ],
      "metadata": {
        "id": "byaxdkjvYPgX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the DataFrame into training, testing, and validation sets\n",
        "train, test, val = new_df[:240000], new_df[240000:262000], new_df[262000:]\n",
        "# Display the class distribution for each set\n",
        "train[\"Class\"].value_counts(), test[\"Class\"].value_counts(), val[\"Class\"].value_counts()"
      ],
      "metadata": {
        "id": "cSPCKv2GGGTb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Converting DataFrames to NumPy Arrays\n",
        "\n",
        "The code converts the DataFrames 'train', 'test', and 'val' into NumPy arrays and prints their shapes.\n"
      ],
      "metadata": {
        "id": "8RZxSOutZZMj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert DataFrames to NumPy arrays\n",
        "train_np, test_np, val_np = train.to_numpy(), test.to_numpy(), val.to_numpy()\n",
        "# Display the shapes of the resulting arrays\n",
        "train_np.shape, test_np.shape, val_np.shape"
      ],
      "metadata": {
        "id": "VtrkCQcpGRnY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Separating Features and Target Variable\n",
        "\n",
        "The code separates the input features and target variable from the NumPy arrays representing the training, testing, and validation sets.\n"
      ],
      "metadata": {
        "id": "jyBgJh6PZv9P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Separate features and target variable for the training set\n",
        "x_train, y_train = train_np[:, :-1], train_np[:, -1]\n",
        "# Separate features and target variable for the testing set\n",
        "x_test, y_test = test_np[:, :-1], test_np[:, -1]\n",
        "# Separate features and target variable for the validation set\n",
        "x_val, y_val = val_np[:, :-1], val_np[:, -1]\n",
        "# Display the shapes of the resulting arrays\n",
        "x_train.shape, y_train.shape, x_test.shape, y_test.shape, x_val.shape, y_val.shape"
      ],
      "metadata": {
        "id": "QKdirzepH3-P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Logistic Regression Model Training and Evaluation\n",
        "\n",
        "The code trains a logistic regression model on the training data and evaluates its accuracy on the same training set.\n"
      ],
      "metadata": {
        "id": "qBNYYL1UaTx4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the Logistic Regression model from scikit-learn\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "# Create a logistic regression model\n",
        "logistic_model = LogisticRegression()\n",
        "# Fit the model to the training data\n",
        "logistic_model.fit(x_train, y_train)\n",
        "# Evaluate the accuracy of the model on the training set\n",
        "logistic_model.score(x_train, y_train)"
      ],
      "metadata": {
        "id": "eLQj7P59IhBP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Result of the Logistic regression on validation\n",
        "\n",
        "- Precision (Fraud): Precision is 0.73 for 'Fraud,' meaning that out of all instances predicted as 'Fraud,' only 73% are true 'Fraud.' The impact of class imbalance is evident here; the model tends to be more conservative in predicting 'Fraud' to avoid false positives.\n",
        "\n",
        "- Recall (Fraud): Recall is 0.53 for 'Fraud,' indicating that only 53% of actual 'Fraud' instances were correctly predicted. The class imbalance affects the model's ability to capture all positive instances.\n",
        "\n",
        "- F1-Score (Fraud): The F1-Score for 'Fraud' is 0.61, providing a balance between precision and recall. However, it reflects the challenge posed by the class imbalance.\n",
        "\n",
        "- Support (Fraud): The number of instances for 'Fraud' is small (36), highlighting the class imbalance issue."
      ],
      "metadata": {
        "id": "xSX-Fvk9bZv8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "print(classification_report(y_val, logistic_model.predict(x_val), target_names=['Not Fraud', 'Fraud']))"
      ],
      "metadata": {
        "id": "9_3vhxheJAeO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Shallow Neural Net\n",
        "This code defines a neural network with one hidden layer (2 units) and uses the ReLU activation function. Batch normalization is applied to normalize the inputs. The output layer has 1 unit with a sigmoid activation function for binary classification. The model is compiled using the Adam optimizer and binary crossentropy loss.\n",
        "\n",
        "During training, a model checkpoint is employed to save the weights of the model with the best performance on the validation set. The training is done for 5 epochs."
      ],
      "metadata": {
        "id": "r5e9u6zNbX9h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import InputLayer, Dense, BatchNormalization\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "\n",
        "# Create a shallow neural network\n",
        "shallow_nn = Sequential()\n",
        "# Add an input layer with the number of features in the input data\n",
        "shallow_nn.add(InputLayer((x_train.shape[1],)))\n",
        "# Add a dense layer with 2 units and ReLU activation function\n",
        "shallow_nn.add(Dense(2, 'relu'))\n",
        "# Add Batch Normalization layer\n",
        "shallow_nn.add(BatchNormalization())\n",
        "# Add the output layer with 1 unit and sigmoid activation function for binary classification\n",
        "shallow_nn.add(Dense(1, 'sigmoid'))\n",
        "\n",
        "# Define a model checkpoint to save the best weights during training\n",
        "checkpoint = ModelCheckpoint('shallow_nn', save_best_only=True)\n",
        "# Compile the model using the Adam optimizer and binary crossentropy loss and use accuracy as a metric\n",
        "shallow_nn.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "RsCATwMsJ9G2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display a summary of the model architecture\n",
        "shallow_nn.summary()"
      ],
      "metadata": {
        "id": "f-YsM7yUOsAT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model on the training set, validate on the validation set, and save the best weights\n",
        "shallow_nn.fit(x_train, y_train, validation_data=(x_val, y_val), epochs=5, callbacks=checkpoint)"
      ],
      "metadata": {
        "id": "2mQtW6_sPBJw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to make predictions using the trained model\n",
        "def neural_net_predictions(model, x):\n",
        "  return (model.predict(x).flatten() > 0.5).astype(int)\n",
        "\n",
        "# neural_net_predictions(shallow_nn, x_val)"
      ],
      "metadata": {
        "id": "2Gma5mEuQYl-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Result Metric\n",
        "\n",
        "- Precision (Fraud): Precision is 0.61 for 'Fraud,' meaning that out of all instances predicted as 'Fraud,' 61% are true 'Fraud.' This is an improvement compared to the logistic regression model.\n",
        "\n",
        "- Recall (Fraud): Recall is 0.78 for 'Fraud,' indicating that 78% of actual 'Fraud' instances were correctly predicted. This is also an improvement compared to the logistic regression model.\n",
        "\n",
        "- F1-Score (Fraud): The F1-Score for 'Fraud' is 0.68, providing a balance between precision and recall.\n",
        "\n",
        "- Support (Fraud): The number of instances for 'Fraud' is small (36), and the model correctly predicts a substantial portion of them.\n",
        "\n",
        "\n",
        "The shallow neural network demonstrates improved performance, especially in terms of precision, recall, and F1-Score for the 'Fraud' class, indicating better handling of the imbalanced dataset."
      ],
      "metadata": {
        "id": "5ryyrAoqeZ__"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(classification_report(y_val, neural_net_predictions(shallow_nn, x_val), target_names=['Not Fraud', 'Fraud']))"
      ],
      "metadata": {
        "id": "Xo4mc_PQROOd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Random Forest Classifier\n",
        "\n",
        "- Precision (Fraud): 0.81 - Out of all instances predicted as 'Fraud,' 81% are true 'Fraud.' This indicates the accuracy of positive predictions.\n",
        "\n",
        "- Recall (Fraud): 0.47 - Only 47% of actual 'Fraud' instances were correctly predicted. This indicates the ability of the model to capture positive instances.\n",
        "\n",
        "- F1-Score (Fraud): 0.60 - The harmonic mean of precision and recall. It provides a balance between precision and recall.\n",
        "\n",
        "The Random Forest classifier shows good overall performance but has a trade-off between precision and recall for the 'Fraud' class. The model tends to be more conservative in predicting 'Fraud,' resulting in a lower recall."
      ],
      "metadata": {
        "id": "uGxRRzgVj1cq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "# Create a Random Forest classifier with a maximum depth of 2 and parallel processing\n",
        "rf = RandomForestClassifier(max_depth=2, n_jobs=-1)\n",
        "# Fit the model to the training data\n",
        "rf.fit(x_train, y_train)\n",
        "\n",
        "print(classification_report(y_val, rf.predict(x_val), target_names=['Not Fraud', 'Fraud']))"
      ],
      "metadata": {
        "id": "Z8AnNoDhURcB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Gradient Boosting Classifier\n",
        "\n",
        "- Precision (Fraud): 0.67 - Precision (Fraud): 0.67 - Out of all instances predicted as 'Fraud,' 67% are true 'Fraud.\n",
        "\n",
        "- Recall (Fraud): 0.67 - 67% of actual 'Fraud' instances were correctly predicted. This indicates the ability of the model to capture positive instances.\n",
        "\n",
        "- F1-Score (Fraud): 0.67 - The harmonic mean of precision and recall. It provides a balance between precision and recall.\n",
        "\n",
        "The model performs exceptionally well on the majority class ('Not Fraud'), achieving perfect precision and recall.\n",
        "\n",
        "For the minority class ('Fraud'), precision and recall are balanced but lower than the majority class.\n",
        "\n",
        "The overall high accuracy might be influenced by the class imbalance, as predicting the majority class accurately contributes significantly."
      ],
      "metadata": {
        "id": "3564FcDHmJjg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "# Create a Gradient Boosting Classifier with specific hyperparameters\n",
        "gbc = GradientBoostingClassifier(n_estimators=50, learning_rate=1.0, max_depth=1, random_state=0)\n",
        "# Fit the model to the training data\n",
        "gbc.fit(x_train, y_train)\n",
        "\n",
        "print(classification_report(y_val, gbc.predict(x_val), target_names=['Not Fraud', 'Fraud']))"
      ],
      "metadata": {
        "id": "LAi9CMCLVVpU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Linear Support Vector Machine classifier\n",
        "\n",
        "- Precision (Fraud): 0.68 - Out of all instances predicted as 'Fraud,' 68% are true 'Fraud.\n",
        "\n",
        "- Recall (Fraud): 0.78 - 78% of actual 'Fraud' instances were correctly predicted.\n",
        "\n",
        "- F1-Score (Fraud): 0.73 - The harmonic mean of precision and recall. It provides a balance between precision and recall.\n",
        "\n",
        "The SVM with a linear kernel and balanced class weights achieved high precision for 'Not Fraud' but relatively lower precision for 'Fraud.' The recall for 'Fraud' is decent, contributing to a balanced F1-Score."
      ],
      "metadata": {
        "id": "7kSNpGyUnuLB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.svm import LinearSVC\n",
        "# Create a Linear Support Vector Machine classifier with balanced class weights\n",
        "svc = LinearSVC(class_weight='balanced')\n",
        "# Fit the model to the training data\n",
        "svc.fit(x_train, y_train)\n",
        "\n",
        "print(classification_report(y_val, svc.predict(x_val), target_names=['Not Fraud', 'Fraud']))"
      ],
      "metadata": {
        "id": "_XXbPbtsWthh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dealing with Class Imbalance in Data\n",
        "\n",
        "### Overview\n",
        "In classification problems, class imbalance can pose challenges as the model may be biased towards the majority class. This imbalance can affect the model's ability to accurately predict the minority class.\n",
        "\n",
        "### Handling Class Imbalance in the Dataset\n",
        "\n",
        "In order to address the issue of class imbalance in the dataset, a strategic approach is taken to create a balanced dataset with equal representation of both 'Fraud' and 'Not Fraud' cases."
      ],
      "metadata": {
        "id": "Sq0pWJjso2ah"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Dataset Division\n",
        "\n",
        "The initial step involves dividing the dataset into two subsets based on the class labels:\n"
      ],
      "metadata": {
        "id": "MHVH9bIssH5s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Code to divide the dataset\n",
        "not_frauds = new_df.query(\"Class == 0\")\n",
        "frauds = new_df.query(\"Class == 1\")\n",
        "# Checking it worked or not\n",
        "not_frauds['Class'].value_counts(), frauds['Class'].value_counts()"
      ],
      "metadata": {
        "id": "o21k2pZoYozr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Creating a Balanced Dataset\n",
        "\n",
        "To address the class imbalance in the original dataset, a balanced dataset is constructed by combining equal instances of 'Fraud' and 'Not Fraud' cases."
      ],
      "metadata": {
        "id": "9B5EmFYtsc48"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Code to create a balanced dataset\n",
        "balanced_df = pd.concat([frauds, not_frauds.sample(len(frauds), random_state=1)])\n",
        "# Check class distribution in the balanced dataset\n",
        "balanced_df['Class'].value_counts()"
      ],
      "metadata": {
        "id": "73lGLRUgZMGw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Shuffling the Balanced Dataset\n",
        "\n",
        "To ensure randomness in the balanced dataset, shuffling is performed. This step is crucial to eliminate any potential ordering bias in the data."
      ],
      "metadata": {
        "id": "6FDs8tjqsvYU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# To shuffle the balanced dataset\n",
        "balanced_df = balanced_df.sample(frac=1, random_state=1)\n",
        "# Display the shuffled balanced dataset\n",
        "balanced_df"
      ],
      "metadata": {
        "id": "PEBG8x58aMFc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Creating Train, Test, and Validation Sets from the Shuffled Balanced Dataset\n",
        "\n",
        "To further prepare the data, subsets for training, testing, and validation are created from the shuffled balanced dataset.\n"
      ],
      "metadata": {
        "id": "E5luaIoMs6wT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Coverting them to numpy array\n",
        "balanced_df_np = balanced_df.to_numpy()\n",
        "# To split the shuffled balanced dataset into train, test, and validation sets\n",
        "x_train_b, y_train_b = balanced_df_np[:700, :-1], balanced_df_np[:700, -1].astype(int)\n",
        "x_test_b, y_test_b = balanced_df_np[700:842, :-1], balanced_df_np[700:842, -1].astype(int)\n",
        "x_val_b, y_val_b = balanced_df_np[842:, :-1], balanced_df_np[842:, -1].astype(int)\n",
        "# Display the shapes of the subsets\n",
        "x_train_b.shape, y_train_b.shape, x_test_b.shape, y_test_b.shape, x_val_b.shape, y_val_b.shape"
      ],
      "metadata": {
        "id": "ILaA3sD6aVO4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Class Distribution in Balanced Train, Test, and Validation Sets\n",
        "\n",
        "Let's examine the class distribution in the balanced training, testing, and validation sets.\n",
        "\n",
        "The output shows the count of instances for each class in the respective sets:\n",
        "\n",
        "Training Set:\n",
        "\n",
        "- Class 0: 347 instances\n",
        "- Class 1: 353 instances\n",
        "\n",
        "Testing Set:\n",
        "\n",
        "- Class 0: 73 instances\n",
        "- Class 1: 69 instances\n",
        "\n",
        "Validation Set:\n",
        "\n",
        "- Class 0: 72 instances\n",
        "- Class 1: 70 instances\n",
        "\n",
        "These counts reflect the balanced nature of the subsets, with an equal representation of both classes."
      ],
      "metadata": {
        "id": "AibvU4mgtc4R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pd.Series(y_train_b).value_counts(), pd.Series(y_test_b).value_counts(), pd.Series(y_val_b).value_counts()"
      ],
      "metadata": {
        "id": "gQCS7i0vbvAi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Logistic Regression on Balanced Data\n",
        "\n",
        "Let's train a Logistic Regression model on the balanced training set (`x_train_b`, `y_train_b`) and evaluate its performance on the balanced validation set (`x_val_b`, `y_val_b`).\n",
        "\n",
        "#### Classification Report for Logistic Regression on Balanced Validation Set\n",
        "\n",
        "The classification report for the Logistic Regression model on the balanced validation set shows promising results:\n",
        "\n",
        "Precision:\n",
        "\n",
        "- For 'Not Fraud': 96%\n",
        "- For 'Fraud': 93%\n",
        "\n",
        "Precision measures the accuracy of the positive predictions made by the model.\n",
        "\n",
        "Recall:\n",
        "\n",
        "- For 'Not Fraud': 93%\n",
        "- For 'Fraud': 96%\n",
        "\n",
        "Recall (Sensitivity) measures the ability of the model to capture all positive instances.\n",
        "\n",
        "F1-Score:\n",
        "\n",
        "- For 'Not Fraud': 94%\n",
        "- For 'Fraud': 94%\n",
        "\n",
        "The F1-score is the harmonic mean of precision and recall, providing a balanced measure.\n",
        "\n",
        "Accuracy:\n",
        "\n",
        "- Overall accuracy of 94%\n",
        "\n",
        "#### Comparison with the Imbalanced Dataset:\n",
        "\n",
        "When comparing these results with those obtained from the imbalanced dataset, the key improvement is seen in the model's ability to correctly predict instances of the minority class ('Fraud'). In imbalanced datasets, models tend to be biased towards the majority class, leading to lower performance on the minority class. In the balanced dataset, precision, recall, and F1-score for both classes are more balanced.\n",
        "\n",
        "It's important to note that the accuracy metric alone might not be a reliable indicator of model performance, especially in imbalanced datasets. The classification report provides a more comprehensive view by considering precision and recall for each class."
      ],
      "metadata": {
        "id": "keftQnWgtzEQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        " # To train Logistic Regression on balanced data\n",
        "logistic_model_b = LogisticRegression()\n",
        "logistic_model_b.fit(x_train_b, y_train_b)\n",
        "# Evaluate the model on the balanced validation set\n",
        "logistic_model_b.score(x_train_b, y_train_b)\n",
        "\n",
        "# Display the classification report for the balanced validation set\n",
        "print(classification_report(y_val_b, logistic_model_b.predict(x_val_b), target_names=['Not Fraud', 'Fraud']))"
      ],
      "metadata": {
        "id": "FRQSo-NYcWWK",
        "collapsed": true,
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Shallow Neural Network on Balanced Data\n",
        "\n",
        "A shallow neural network is trained on the balanced training set (`x_train_b`, `y_train_b`) and evaluated on the balanced validation set (`x_val_b`, `y_val_b`).\n",
        "\n",
        "#### Classification Report for Shallow Neural Network on Balanced Validation Set\n",
        "\n",
        "The classification report for the shallow neural network on the balanced validation set is as follows:\n",
        "\n",
        "#### Interpretation:\n",
        "\n",
        "- **Precision:**\n",
        "  - For 'Not Fraud': 95%\n",
        "  - For 'Fraud': 96%\n",
        "\n",
        "- **Recall:**\n",
        "  - For 'Not Fraud': 96%\n",
        "  - For 'Fraud': 94%\n",
        "\n",
        "- **F1-Score:**\n",
        "  - For 'Not Fraud': 95%\n",
        "  - For 'Fraud': 95%\n",
        "\n",
        "- **Accuracy:**\n",
        "  - Overall accuracy of 95%\n",
        "\n",
        "These metrics indicate a balanced performance, with a notable ability to correctly identify instances of both classes. Compared to the logistic regression model, the shallow neural network shows competitive results and may offer advantages in capturing complex patterns in the data.\n",
        "\n",
        "It's important to consider the trade-offs between precision and recall based on the specific goals of the classification task. Further fine-tuning or exploration of more complex models may lead to even better performance."
      ],
      "metadata": {
        "id": "OGKVDRRt0kJT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# To create and train a shallow neural network on balanced data\n",
        "shallow_nn_b = Sequential()\n",
        "shallow_nn_b.add(InputLayer((x_train_b.shape[1],)))\n",
        "shallow_nn_b.add(Dense(2, 'relu'))\n",
        "shallow_nn_b.add(BatchNormalization())\n",
        "shallow_nn_b.add(Dense(1, 'sigmoid'))\n",
        "\n",
        "checkpoint_b = ModelCheckpoint('shallow_nn_b', save_best_only=True)\n",
        "shallow_nn_b.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Display the summary of the model architecture\n",
        "shallow_nn_b.summary()\n",
        "\n",
        "# Train the model on balanced data and save the best weights\n",
        "shallow_nn_b.fit(x_train_b, y_train_b, validation_data=(x_val_b, y_val_b), epochs=40, callbacks=checkpoint)\n",
        "\n",
        "\n",
        "print(classification_report(y_val_b, neural_net_predictions(shallow_nn_b, x_val_b), target_names=['Not Fraud', 'Fraud']))"
      ],
      "metadata": {
        "id": "WziE9p3Jcug-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Random Forest Classifier on Balanced Data\n",
        "\n",
        "A Random Forest classifier is trained on the balanced training set (`x_train_b`, `y_train_b`).\n",
        "\n",
        "#### Classification Report for Random Forest Classifier on Balanced Validation Set\n",
        "\n",
        "The classification report for the Random Forest classifier on the balanced validation set is as follows:\n",
        "\n",
        "\n",
        "### Interpretation:\n",
        "\n",
        "- **Precision:**\n",
        "  - For 'Not Fraud': 93%\n",
        "  - For 'Fraud': 97%\n",
        "\n",
        "- **Recall:**\n",
        "  - For 'Not Fraud': 97%\n",
        "  - For 'Fraud': 93%\n",
        "\n",
        "- **F1-Score:**\n",
        "  - For 'Not Fraud': 95%\n",
        "  - For 'Fraud': 95%\n",
        "\n",
        "- **Accuracy:**\n",
        "  - Overall accuracy of 95%\n",
        "\n",
        "The Random Forest classifier demonstrates a strong ability to correctly classify both classes. The precision, recall, and F1-score for both 'Not Fraud' and 'Fraud' are balanced, contributing to an impressive overall accuracy of 94%.\n",
        "\n",
        "Comparing these results with the Shallow Neural Network and Logistic Regression, the Random Forest model showcases competitive performance and is well-suited for the balanced dataset.\n",
        "\n"
      ],
      "metadata": {
        "id": "htDkazEU11O9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# To create and train a Random Forest Classifier on balanced data\n",
        "rf_b = RandomForestClassifier(max_depth=2, n_jobs=-1)\n",
        "rf_b.fit(x_train_b, y_train_b)\n",
        "\n",
        "print(classification_report(y_val_b, rf_b.predict(x_val_b), target_names=['Not Fraud', 'Fraud']))"
      ],
      "metadata": {
        "id": "9-c3TNuKc5Z8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Gradient Boosting Classifier on Balanced Data\n",
        "\n",
        "A Gradient Boosting Classifier is trained on the balanced training set (`x_train_b`, `y_train_b`).\n",
        "\n",
        "#### Classification Report for Gradient Boosting Classifier on Balanced Validation Set\n",
        "\n",
        "The classification report for the Gradient Boosting Classifier on the balanced validation set is as follows:\n",
        "\n",
        "\n",
        "#### Interpretation:\n",
        "\n",
        "- **Precision:**\n",
        "  - For 'Not Fraud': 98%\n",
        "  - For 'Fraud': 87%\n",
        "\n",
        "- **Recall:**\n",
        "  - For 'Not Fraud': 86%\n",
        "  - For 'Fraud': 99%\n",
        "\n",
        "- **F1-Score:**\n",
        "  - For 'Not Fraud': 92%\n",
        "  - For 'Fraud': 93%\n",
        "\n",
        "- **Accuracy:**\n",
        "  - Overall accuracy of 92%\n",
        "\n",
        "These metrics indicate a well-performing Gradient Boosting Classifier on the balanced dataset. The model demonstrates a high ability to correctly classify instances of both 'Not Fraud' and 'Fraud', contributing to a balanced F1-score and accuracy.\n",
        "\n"
      ],
      "metadata": {
        "id": "4jCOq43F29Bc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# To create and train a Gradient Boosting Classifier on balanced data\n",
        "gbc_b = GradientBoostingClassifier(n_estimators=50, learning_rate=1.0, max_depth=1, random_state=0)\n",
        "gbc_b.fit(x_train_b, y_train_b)\n",
        "\n",
        "print(classification_report(y_val_b, gbc_b.predict(x_val_b), target_names=['Not Fraud', 'Fraud']))"
      ],
      "metadata": {
        "id": "w1yQ73JPc8TF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Linear Support Vector Machine classifier on Balanced Data\n",
        "\n",
        "A Linear Support Vector Machine classifier is trained on the balanced training set (`x_train_b`, `y_train_b`).\n",
        "\n",
        "#### Classification Report for Linear Support Vector Machine classifier on Balanced Validation Set\n",
        "\n",
        "The classification report for the Linear Support Vector Machine classifier on the balanced validation set is as follows:\n",
        "\n",
        "\n",
        "#### Interpretation:\n",
        "\n",
        "- **Precision:**\n",
        "  - For 'Not Fraud': 96%\n",
        "  - For 'Fraud': 93%\n",
        "\n",
        "- **Recall:**\n",
        "  - For 'Not Fraud': 93%\n",
        "  - For 'Fraud': 96%\n",
        "\n",
        "- **F1-Score:**\n",
        "  - For 'Not Fraud': 94%\n",
        "  - For 'Fraud': 94%\n",
        "\n",
        "- **Accuracy:**\n",
        "  - Overall accuracy of 94%\n",
        "\n",
        "These metrics indicate a strong performance by the Linear SVM classifier on the balanced dataset. The model demonstrates a balanced ability to correctly classify instances of both 'Not Fraud' and 'Fraud', contributing to a high F1-score and accuracy.\n"
      ],
      "metadata": {
        "id": "JSJWNTUT35di"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# To create and train a Linear Support Vector Machine classifier on balanced data\n",
        "svc_b = LinearSVC()\n",
        "svc_b.fit(x_train_b, y_train_b)\n",
        "\n",
        "print(classification_report(y_val_b, svc_b.predict(x_val_b), target_names=['Not Fraud', 'Fraud']))"
      ],
      "metadata": {
        "id": "M-5nUKqtc-aB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Test Metrics for Logistic Regression on Balanced Test Set\n",
        "\n",
        "The test metrics for the Logistic Regression model on the balanced test set are as follows:\n",
        "\n",
        "\n",
        "#### Interpretation:\n",
        "\n",
        "- **Precision:**\n",
        "  - For 'Not Fraud': 92%\n",
        "  - For 'Fraud': 95%\n",
        "\n",
        "- **Recall:**\n",
        "  - For 'Not Fraud': 96%\n",
        "  - For 'Fraud': 91%\n",
        "\n",
        "- **F1-Score:**\n",
        "  - For 'Not Fraud': 94%\n",
        "  - For 'Fraud': 93%\n",
        "\n",
        "- **Accuracy:**\n",
        "  - Overall accuracy of 94%\n",
        "\n",
        "These metrics indicate that the Logistic Regression model generalizes well to the balanced test set, maintaining a balanced performance in correctly classifying instances of both 'Not Fraud' and 'Fraud'."
      ],
      "metadata": {
        "id": "Ni-3qEuy4eld"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Test\n",
        "print(\"Logistic Metric on Test\")\n",
        "print(classification_report(y_test_b, logistic_model_b.predict(x_test_b), target_names=['Not Fraud', 'Fraud']))"
      ],
      "metadata": {
        "id": "h_4bZi1ggjHK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Test Metrics for Shallow Neural Network on Balanced Test Set\n",
        "\n",
        "The test metrics for the Shallow Neural Network on the balanced test set are as follows:\n",
        "\n",
        "\n",
        "#### Interpretation:\n",
        "\n",
        "- **Precision:**\n",
        "  - For 'Not Fraud': 92%\n",
        "  - For 'Fraud': 97%\n",
        "\n",
        "- **Recall:**\n",
        "  - For 'Not Fraud': 97%\n",
        "  - For 'Fraud': 91%\n",
        "\n",
        "- **F1-Score:**\n",
        "  - For 'Not Fraud': 95%\n",
        "  - For 'Fraud': 94%\n",
        "\n",
        "- **Accuracy:**\n",
        "  - Overall accuracy of 94%\n",
        "\n",
        "These metrics indicate the performance of the Shallow Neural Network on the balanced test set. The model demonstrates a reasonable ability to correctly classify instances of both 'Not Fraud' and 'Fraud', contributing to a balanced F1-score and accuracy."
      ],
      "metadata": {
        "id": "c8UmEHkb4iaK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Shallow Neural Net Metric on Test\")\n",
        "print(classification_report(y_test_b, neural_net_predictions(shallow_nn_b ,x_test_b), target_names=['Not Fraud', 'Fraud']))"
      ],
      "metadata": {
        "id": "mX0rUKlNg2wW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Test Metrics for Random Forest Classifier on Balanced Test Set\n",
        "\n",
        "The test metrics for the Random Forest Classifier on the balanced test set are as follows:\n",
        "\n",
        "\n",
        "#### Interpretation:\n",
        "\n",
        "- **Precision:**\n",
        "  - For 'Not Fraud': 90%\n",
        "  - For 'Fraud': 100%\n",
        "\n",
        "- **Recall:**\n",
        "  - For 'Not Fraud': 100%\n",
        "  - For 'Fraud': 88%\n",
        "\n",
        "- **F1-Score:**\n",
        "  - For 'Not Fraud': 95%\n",
        "  - For 'Fraud': 94%\n",
        "\n",
        "- **Accuracy:**\n",
        "  - Overall accuracy of 94%\n",
        "\n",
        "These metrics demonstrate the performance of the Random Forest Classifier on the balanced test set. The model exhibits a balanced ability to correctly classify instances of both 'Not Fraud' and 'Fraud', contributing to a high F1-score and accuracy.\n"
      ],
      "metadata": {
        "id": "zL_63urt4kT-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"RandomForest Classifier Metric on Test\")\n",
        "print(classification_report(y_test_b, rf_b.predict(x_test_b), target_names=['Not Fraud', 'Fraud']))"
      ],
      "metadata": {
        "id": "4MhEDv7ag3UP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Test Metrics for Gradient Boosting Classifier on Balanced Test Set\n",
        "\n",
        "The test metrics for the Gradient Boosting Classifier on the balanced test set are as follows:\n",
        "\n",
        "#### Interpretation:\n",
        "\n",
        "- **Precision:**\n",
        "  - For 'Not Fraud': 92%\n",
        "  - For 'Fraud': 90%\n",
        "\n",
        "- **Recall:**\n",
        "  - For 'Not Fraud': 90%\n",
        "  - For 'Fraud': 91%\n",
        "\n",
        "- **F1-Score:**\n",
        "  - For 'Not Fraud': 91%\n",
        "  - For 'Fraud': 91%\n",
        "\n",
        "- **Accuracy:**\n",
        "  - Overall accuracy of 91%\n",
        "\n",
        "These metrics illustrate the performance of the Gradient Boosting Classifier on the balanced test set. The model displays a balanced ability to correctly classify instances of both 'Not Fraud' and 'Fraud', resulting in a high F1-score and accuracy."
      ],
      "metadata": {
        "id": "mChQwVF_4jm5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Gradient Boosting Classifier Metric on Test\")\n",
        "print(classification_report(y_test_b, gbc_b.predict(x_test_b), target_names=['Not Fraud', 'Fraud']))"
      ],
      "metadata": {
        "id": "SOWoBoY7hNgZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Test Metrics for Linear Support Vector Machine (LinearSVC) on Balanced Test Set\n",
        "\n",
        "The test metrics for the Linear Support Vector Machine (LinearSVC) on the balanced test set are as follows:\n",
        "\n",
        "#### Interpretation:\n",
        "\n",
        "- **Precision:**\n",
        "  - For 'Not Fraud': 92%\n",
        "  - For 'Fraud': 94%\n",
        "\n",
        "- **Recall:**\n",
        "  - For 'Not Fraud': 95%\n",
        "  - For 'Fraud': 91%\n",
        "\n",
        "- **F1-Score:**\n",
        "  - For 'Not Fraud': 93%\n",
        "  - For 'Fraud': 93%\n",
        "\n",
        "- **Accuracy:**\n",
        "  - Overall accuracy of 93%\n",
        "\n",
        "These metrics showcase the performance of the Linear Support Vector Machine (LinearSVC) on the balanced test set. The model demonstrates a balanced ability to correctly classify instances of both 'Not Fraud' and 'Fraud', resulting in a high F1-score and accuracy.\n"
      ],
      "metadata": {
        "id": "tZ94b1et4lnT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"LinearSVC Metric on Test\")\n",
        "print(classification_report(y_test_b, svc_b.predict(x_test_b), target_names=['Not Fraud', 'Fraud']))"
      ],
      "metadata": {
        "id": "ZLPwTB1ShN7_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Overall Observations\n",
        "\n",
        "- The models demonstrate competitive performance on the balanced dataset, with balanced precision, recall, and F1-scores for both classes.\n",
        "- Random Forest and Shallow Neural Network exhibit excellent precision for the 'Fraud' class.\n",
        "- Logistic Regression and LinearSVC show strong overall accuracy and balanced performance.\n",
        "- The choice of the best model depends on the specific goals and trade-offs between precision and recall.\n",
        "\n",
        "# Factor Influencing Model Selection\n",
        "\n",
        "The considerations and factors involved in choosing the best model based on the specific goals and trade-offs between precision and recall:\n",
        "\n",
        "## Precision and Recall:\n",
        "- **Precision:** Precision is the ratio of true positive predictions to the total predicted positives. It focuses on minimizing false positives. In the context of fraud detection, a high precision means that when the model predicts a transaction as fraudulent, it is highly likely to be accurate.\n",
        "\n",
        "- **Recall:** Recall is the ratio of true positive predictions to the total actual positives. It focuses on minimizing false negatives. In fraud detection, high recall implies that the model is effective in identifying most of the actual fraudulent transactions.\n",
        "\n",
        "## Trade-offs:\n",
        "### 1. Precision-Focused Approach:\n",
        "   - **Scenario:** If the business priority is to minimize the number of false positives (genuine transactions misclassified as fraud), a precision-focused approach is appropriate.\n",
        "   - **Implication:** This ensures that when the model flags a transaction as fraudulent, it is very likely to be an actual case of fraud. However, it may result in missing some actual fraud cases (lower recall).\n",
        "\n",
        "### 2. Recall-Focused Approach:\n",
        "   - **Scenario:** If the business is more concerned about capturing as many fraud cases as possible, even at the cost of a higher false positive rate, a recall-focused approach is suitable.\n",
        "   - **Implication:** This maximizes the identification of actual fraud cases but may lead to more false positives, as the model might be less stringent in its criteria for flagging transactions.\n",
        "\n",
        "## Model Considerations:\n",
        "### 1. Precision-Focused Models:\n",
        "   - **Logistic Regression:** Known for its simplicity and interpretability, logistic regression can be tuned to prioritize precision.\n",
        "   - **Shallow Neural Network:** With careful parameter tuning, a neural network can be designed to emphasize precision, leveraging its ability to capture complex patterns.\n",
        "\n",
        "### 2. Recall-Focused Models:\n",
        "   - **Linear Support Vector Machine (LinearSVC):** Linear models like LinearSVC can be effective in recall-focused scenarios while maintaining interpretability.\n",
        "   - **Ensemble Models (Random Forest, Gradient Boosting):** These models, especially Gradient Boosting, are capable of capturing complex relationships, leading to higher recall.\n",
        "\n",
        "## Business Decision:\n",
        "- **Precision-Recall Trade-off:** The business decision hinges on the acceptable trade-off between precision and recall. It's about finding the right balance based on the business's risk tolerance and the cost associated with false positives and false negatives.\n",
        "\n",
        "- **Consideration of Business Goals:** The choice of the best model should align with the overarching business goals. For instance, a financial institution might prioritize minimizing false positives to avoid inconveniencing legitimate customers, while an e-commerce platform might prioritize recall to prevent fraudulent transactions.\n",
        "\n",
        "In essence, the decision-making process involves a thoughtful evaluation of the specific business context, risks, and priorities. It's not a one-size-fits-all scenario, and the chosen model should align with the unique requirements and constraints of the business at hand."
      ],
      "metadata": {
        "id": "ttOgKrrc6Ntn"
      }
    }
  ]
}